{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Capstone Project\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The purpose of the project is to build a data warehouse used to analyze the factors affecting the decision to choose a new place to live when immigrating to the US of people from all over the world. The project only analyzes based on data of air migrants.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "|Data Set|Format|Description|\n",
    "|--------|------|-----------|\n",
    "|I94 Immigration Data|SAS\t|Contains international visitor arrival statistics by world regions and select countries, type of visa, transportation mode, age, arrived states, entry ports.|\n",
    "|Airport Code Data|CSV| Contains airport codes and corresponding cities.|\n",
    "|U.S. City Demographic Data\t|CSV\t|Contains information about the demographics of all US cities.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import all needed libraries \n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1. airport-codes_csv.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of airport_codes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to read csv file\n",
    "airport_codes = pd.read_csv('airport-codes_csv.csv')\n",
    "\n",
    "# Overview data\n",
    "print('First 5 rows of airport_codes:')\n",
    "airport_codes.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# get some basic informations about airport_codes (the # of rows, the # of cols, null count, col's datatype)\n",
    "airport_codes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count duplicated rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if it has any duplicated rows\n",
    "print('Count duplicated rows:')\n",
    "airport_codes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count duplicated of primary key --ident--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if it has any duplicated values in the column that will be primary key\n",
    "print('Count duplicated of primary key --ident--')\n",
    "airport_codes.duplicated(subset = ['ident']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2. us-cities-demographics.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of demographics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to read csv file\n",
    "demographics = pd.read_csv('us-cities-demographics.csv', delimiter = ';')\n",
    "\n",
    "# Overview data\n",
    "print('First 5 rows of demographics:')\n",
    "demographics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# get some basic informations about demographics (the # of rows, the # of cols, null count, col's datatype)\n",
    "demographics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count duplicated rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if it has any duplicated rows\n",
    "print('Count duplicated rows:')\n",
    "demographics.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count duplicated of primary key --'City', 'State Code', 'Race'--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if it has any duplicated values in the columns\n",
    "print(\"Count duplicated of primary key --'City', 'State Code', 'Race'--\")\n",
    "demographics.duplicated(subset = ['City', 'State Code', 'Race']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count duplicated of primary key --'City', 'State Code', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'Race'--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if it has any duplicated values in the columns\n",
    "print(\"Count duplicated of primary key --'City', 'State Code', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'Race'--\")\n",
    "demographics.duplicated(subset = ['City', 'State Code', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'Race']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3. immigration_data_sample.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use pyspark to read parquet files\n",
    "immigrations = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "# write to parquet\n",
    "immigrations.write.mode('overwrite').parquet(\"sas_data\")\n",
    "# read from parquet\n",
    "immigrations = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of demographics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1'),\n",
       " Row(cicid=5748518.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='NV', depdate=20591.0, i94bir=32.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1984.0, dtaddto='10292016', gender='F', insnum=None, airline='VA', admnum=94955622830.0, fltno='00007', visatype='B1'),\n",
       " Row(cicid=5748519.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20582.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956406530.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748520.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='F', insnum=None, airline='DL', admnum=94956451430.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748521.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=28.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956388130.0, fltno='00040', visatype='B1')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview data\n",
    "print('First 5 rows of demographics:')\n",
    "immigrations.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row count\n",
    "immigrations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Datatype of columns\n",
    "immigrations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count duplicated of primary key --cicid--\n",
      "+-----+\n",
      "|cicid|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if it has any duplicated values in the column that will be primary key\n",
    "# refer to https://www.datasciencemadesimple.com/get-duplicate-rows-in-pyspark/\n",
    "print(\"Count duplicated of primary key --cicid--\")\n",
    "duplicates = immigrations.groupBy('cicid').count().filter('count > 1')\n",
    "duplicates.drop('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3. Cleaning Data\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1. Drop duplicated rows\n",
    "Because there's no duplicates in airport_codes, demographics, immigrations. Therefore no need to drop duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2. Drop rows have NULL\n",
    "Because there's no NULL in airport_codes, demographics, immigrations. Therefore no need to drop any rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3. Transform data\n",
    "Because arrdate & depdate in immigrations are sas-formatted values. Therefore need to transform these columns to datetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_sasformat_to_date(sas_num):\n",
    "    '''\n",
    "        Description:\n",
    "            This func uses to convert date columns have SAS-formatted values to DateType ('yyyy/mm/dd')\n",
    "            Params: \n",
    "                - sas_num: a SAS-formatted value\n",
    "            Return: date value ('yyyy/mm/dd')\n",
    "            The solution is refered from https://stackoverflow.com/questions/36412864/convert-numeric-sas-date-to-datetime-in-pandas\n",
    "    '''\n",
    "    if sas_num is not None:\n",
    "        return pd.to_timedelta(sas_num, unit='D') + pd.Timestamp('1960-1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#  Convert to udf to use with spark\n",
    "# refer to https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html\n",
    "udf_convert_sasformat_to_date = udf(convert_sasformat_to_date, DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform sas format of arrdate to date type\n",
    "immigrations = immigrations.withColumn('arrdate', udf_convert_sasformat_to_date(col('arrdate')))\n",
    "\n",
    "# transform sas format of depdate to date type\n",
    "immigrations = immigrations.withColumn('depdate', udf_convert_sasformat_to_date(col('depdate')))\n",
    "\n",
    "# transform sas format of i94bir to date type\n",
    "immigrations = immigrations.withColumn('i94bir', udf_convert_sasformat_to_date(col('i94bir')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get immigrations have been by airport only\n",
    "immigrations_air = immigrations[immigrations['i94mode'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get US's airport only\n",
    "airport_codes_us = airport_codes[airport_codes['iso_country'] == 'US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# get state code only from iso_region field\n",
    "airport_codes_us['iso_region'] = airport_codes_us['iso_region'].str[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mapping_city_code():\n",
    "    with open(\"I94_SAS_Labels_Descriptions.SAS\") as f:\n",
    "        contents = f.readlines()\n",
    "    \n",
    "    cities = {}\n",
    "    cicos = []\n",
    "    cinas = []\n",
    "    stcos = []\n",
    "    \n",
    "    for row in contents[303:817]:\n",
    "        # city_code = pair[0], city_name & state_code = pair[1]\n",
    "        pair = row.split('=')\n",
    "        # to split city_name and state_code\n",
    "        cist_pair = pair[1].split(',')\n",
    "        \n",
    "        cico, cina = pair[0].strip(\"\\t\").strip().strip(\"'\"), cist_pair[0].strip(\"\\t\").strip().strip(\"'\")\n",
    "        if len(cist_pair) == 2:\n",
    "            stco = cist_pair[1].strip(\"\\n\").strip().strip(\"'\")\n",
    "        else:\n",
    "            stco = ''\n",
    "            \n",
    "        cicos.append(cico)\n",
    "        cinas.append(cina)\n",
    "        stcos.append(stco)\n",
    "        df_cities = pd.DataFrame({'city_code' : cicos,\n",
    "                                'city_name' : cinas,\n",
    "                                'state_code' : stcos }, \n",
    "                                columns=['city_code', 'city_name', 'state_code'])\n",
    "        \n",
    "    return df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cities = mapping_city_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_states = demographics[['State Code', 'State']].drop_duplicates()\n",
    "df_states['State Code'] = df_states['State Code'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "![data model](schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.1 Conceptual Data Model\n",
    "The data model consists of 4 tables (1 fact & 3 dims):\n",
    "- FACT_IMMIGRATION: contains all immigration events and date\n",
    "- DIM_IMMIGRANT: contains immigrant information\n",
    "- DIM_AIRPORT: contains airport information\n",
    "- DIM_STATE_DEMOGRAPHIC: contains US states demographic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Load data to dim, fact tables and change clearer column name.\n",
    "- FACT_IMMIGRATION: get fields from files in sas_data folder\n",
    "- DIM_IMMIGRANT: get fields from files in sas_data folder\n",
    "- DIM_AIRPORT: get fields from airport-codes_csv.csv file\n",
    "- DIM_STATE_DEMOGRAPHIC: get fields from us-cities-demographics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DIM_CITY\n",
    "def create_dim_city():\n",
    "    dim_city = df_cities.join(df_states.set_index('State Code'), on='state_code', how = 'left')\n",
    "    dim_city.columns = ['city_code', 'city_name', 'state_code', 'state_name']\n",
    "    \n",
    "    # make sure primary key is unique\n",
    "    dim_city = dim_city.drop_duplicates(['city_code', 'state_code'])\n",
    "    \n",
    "    return dim_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# prepare a temp dim_city to get the corresponding city_code for dim_airport\n",
    "dim_city_tojoin_airport = create_dim_city().drop_duplicates(['city_code', 'city_name'])\n",
    "dim_city_tojoin_demog = create_dim_city().drop_duplicates(['city_code', 'city_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DIM_AIRPORT\n",
    "def create_dim_airport():\n",
    "    # get needed fields from airport-codes_csv.csv file\n",
    "    dim_airport = airport_codes_us[['ident', 'iso_region', 'municipality', 'name', 'type', 'elevation_ft', 'gps_code', 'local_code', 'coordinates']]\n",
    "    \n",
    "    # setup clearer column name\n",
    "    dim_airport.columns = ['airport_ident', 'state_code', 'city_name', 'name', 'type', 'elevation_ft', 'gps_code', 'local_code', 'coordinates']\n",
    "    \n",
    "    dim_airport['city_name'] = dim_airport['city_name'].str.upper()\n",
    "    \n",
    "    # join to temp dim_city to extract city_code\n",
    "    dim_airport = dim_airport.merge(dim_city_tojoin_airport.set_index('city_name'), on='city_name', how = 'inner')\n",
    "    dim_airport = dim_airport[['airport_ident', 'city_code', 'state_code_x', 'name', 'type', 'elevation_ft', 'gps_code', 'local_code', 'coordinates']]\n",
    "    dim_airport.columns = ['airport_ident', 'city_code', 'state_code', 'name', 'type', 'elevation_ft', 'gps_code', 'local_code', 'coordinates']\n",
    "    \n",
    "    # make sure primary key is unique\n",
    "    dim_airport = dim_airport.drop_duplicates(['airport_ident'])\n",
    "    \n",
    "    return dim_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DIM_DEMOGRAPHIC\n",
    "def create_dim_demographic():\n",
    "    # includes only US demographic    \n",
    "    # get needed fields from us-cities-demographics.csv\n",
    "    dim_demographic = demographics[['City', 'State Code', 'State', 'Race', 'Count', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size']]\n",
    "    dim_demographic.columns = ['city_name', 'state_code', 'state', 'race', 'count_each_race', 'median_age', 'male_population', 'female_population', 'total_population', 'num_vetarans', 'foreign_born', 'avg_household_size']\n",
    "    \n",
    "    dim_demographic['city_name'] = dim_demographic['city_name'].str.upper()\n",
    "    \n",
    "    # join to temp dim_city to extract city_code\n",
    "    dim_demographic = dim_demographic.merge(dim_city_tojoin_demog.set_index('city_name'), on='city_name', how = 'inner')\n",
    "    dim_demographic = dim_demographic[['city_code', 'state_code_x', 'race', 'count_each_race', 'median_age', 'male_population', 'female_population', 'total_population', 'num_vetarans', 'foreign_born', 'avg_household_size']]\n",
    "    dim_demographic.columns = ['city_code', 'state_code', 'race', 'count_each_race', 'median_age', 'male_population', 'female_population', 'total_population', 'num_vetarans', 'foreign_born', 'avg_household_size']\n",
    "    \n",
    "    # make sure primary key is unique\n",
    "    dim_demographic = dim_demographic.drop_duplicates(['city_code', 'state_code', 'race'])\n",
    "    \n",
    "    return dim_demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# FACT_IMMGRATION\n",
    "def create_fact_immigration():\n",
    "    fact_immigration = immigrations_air.select(\n",
    "        (immigrations_air.cicid.cast(IntegerType())).alias('cic_id'),\n",
    "        immigrations_air.i94port.alias('city_code'),\n",
    "        immigrations_air.i94addr.alias('state_code'),\n",
    "        immigrations_air.depdate.alias('departure_date'),\n",
    "        immigrations_air.arrdate.alias('arrive_date'),\n",
    "        (immigrations_air.i94mode.cast(IntegerType())).alias('travel_mode'),\n",
    "        (immigrations_air.i94visa.cast(IntegerType())).alias('visa_type_id'),\n",
    "        immigrations_air.visatype.alias('visa_type'),\n",
    "        immigrations_air.airline,\n",
    "        (immigrations_air.admnum.cast(IntegerType())).alias('admin_num'),\n",
    "        immigrations_air.fltno.alias('flight_no'),\n",
    "        (immigrations_air.admnum.cast(IntegerType())).alias('admin_num'),\n",
    "        immigrations_air.i94bir.alias('birthday'),\n",
    "        immigrations_air.gender\n",
    "    )\n",
    "    \n",
    "    return fact_immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Run the create function to create and load data to dim, fact tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "dim_city = create_dim_city()\n",
    "dim_airport = create_dim_airport()\n",
    "dim_demographic = create_dim_demographic()\n",
    "fact_immigration = create_fact_immigration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_code</th>\n",
       "      <th>city_name</th>\n",
       "      <th>state_code</th>\n",
       "      <th>state_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE</td>\n",
       "      <td>AK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DTH</td>\n",
       "      <td>DUTCH HARBOR</td>\n",
       "      <td>AK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EGL</td>\n",
       "      <td>EAGLE</td>\n",
       "      <td>AK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  city_code                 city_name       state_code state_name\n",
       "0       BAR  BAKER AAF - BAKER ISLAND               AK     Alaska\n",
       "1       DAC             DALTONS CACHE          AK             NaN\n",
       "2       PIZ    DEW STATION PT LAY DEW               AK     Alaska\n",
       "3       DTH              DUTCH HARBOR         AK              NaN\n",
       "4       EGL                     EAGLE  AK                     NaN"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_city.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dim_city: 514 rows.\n",
      "- dim_airport: 2800 rows.\n",
      "- dim_demographic: 691 rows.\n",
      "- fact_immigration: 2994505 rows.\n"
     ]
    }
   ],
   "source": [
    "# Check if there're empty tables\n",
    "dc_all = len(dim_city)\n",
    "da_all = len(dim_airport)\n",
    "dd_all = len(dim_demographic)\n",
    "fi_all = fact_immigration.count()\n",
    "\n",
    "print(f'- dim_city: {dc_all} rows.')\n",
    "print(f'- dim_airport: {da_all} rows.')\n",
    "print(f'- dim_demographic: {dd_all} rows.')\n",
    "print(f'- fact_immigration: {fi_all} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary key of dim_city is unique.\n",
      "The primary key of dim_airport is unique.\n",
      "The primary key of dim_demographic is unique.\n",
      "The primary key of fact_immigration is unique.\n"
     ]
    }
   ],
   "source": [
    "# Check the primary key is unique\n",
    "dc_unique = len(dim_city[['city_code', 'state_code']].drop_duplicates())\n",
    "da_unique = len(dim_airport.drop_duplicates(['airport_ident']))\n",
    "dd_unique = len(dim_demographic.drop_duplicates(['city_code', 'state_code', 'race']))\n",
    "fi_unique = fact_immigration.dropDuplicates(['cic_id']).count()\n",
    "\n",
    "if dc_all == dc_unique:\n",
    "    print('The primary key of dim_city is unique.')\n",
    "else:\n",
    "    print('dim_city: Fail TC.')\n",
    "\n",
    "if da_all == da_unique:\n",
    "    print('The primary key of dim_airport is unique.')\n",
    "else:\n",
    "    print('dim_airport: Fail TC.')\n",
    "    \n",
    "if dd_all == dd_unique:\n",
    "    print('The primary key of dim_demographic is unique.')\n",
    "else:\n",
    "    print('dim_demographic: Fail TC.')\n",
    "    \n",
    "if fi_all == fi_unique:\n",
    "    print('The primary key of fact_immigration is unique.')\n",
    "else: \n",
    "    print('fact_immigration: Fail TC.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check The ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_ident</th>\n",
       "      <th>city_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>name</th>\n",
       "      <th>race</th>\n",
       "      <th>count_each_race</th>\n",
       "      <th>median_age</th>\n",
       "      <th>total_population</th>\n",
       "      <th>avg_household_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN78</td>\n",
       "      <td>GAR</td>\n",
       "      <td>IN</td>\n",
       "      <td>Police Heliport</td>\n",
       "      <td>Asian</td>\n",
       "      <td>537</td>\n",
       "      <td>38.1</td>\n",
       "      <td>77354</td>\n",
       "      <td>2.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_ident city_code state_code             name   race  count_each_race  \\\n",
       "2          IN78       GAR         IN  Police Heliport  Asian              537   \n",
       "\n",
       "   median_age  total_population  avg_household_size  \n",
       "2        38.1             77354                2.35  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the data to answer the question: Where is Police Heliport airport located? And How is Asian's demographic situation in here?\n",
    "output = pd.merge(dim_airport, dim_demographic, on=['city_code', 'state_code'], how='inner')\n",
    "output = output[(output['name'] == 'Police Heliport') & (output['race'] == 'Asian')]\n",
    "output = output[['airport_ident', 'city_code', 'state_code', 'name', 'race', 'count_each_race', 'median_age', 'total_population', 'avg_household_size']]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3. Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- DIM_CITY\n",
    "\n",
    "|COLUMN NAME|DESCRIPTION|\n",
    "|-----------|-----------|\n",
    "|city_code|3 digit for the city code that is in United State. city_code and state_code are combined to composite key as primary key of DIM_CITY|\n",
    "|state_code|3 digit for the state code that is in United State. city_code and state_code are combined to composite key as primary key of DIM_CITY|\n",
    "|city_name|city name|\n",
    "|state_name|state name|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- DIM_AIRPORT\n",
    "\n",
    "|COLUMN NAME|DESCRIPTION|\n",
    "|-----------|-----------|\n",
    "|airport_ident|3-4 digit for the airport identity code that is in only United State. And primary key of DIM_AIRPORT also.\n",
    "|city_code|3 digit for the city code that is in United State. city_code and state_code are combined to composite key as foreign key reference to DIM_CITY|\n",
    "|state_code|3 digit for the state code that is in United State. city_code and state_code are combined to composite key as foreign key reference to DIM_CITY|\n",
    "|name|airport name|\n",
    "|type|airport type: small, heliport, closed|\n",
    "|elevation_ft| the highest point of the landing area|\n",
    "|gps_code|gps code is unique|\n",
    "|local_code|local code is unique|\n",
    "|coordinates|are formed by two components that are a latitude , giving the north-south position, and a longitude, giving the east-west position|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- DIM_DEMOGRAPHIC\n",
    "\n",
    "|COLUMN NAME|DESCRIPTION|\n",
    "|-----------|-----------|\n",
    "|city_code|3 digit for the city code that is in United State. city_code and state_code and race are combined to composite key as primary key of DIM_DEMOGRAPHIC|\n",
    "|state_code|3 digit for the state code that is in United State. city_code and state_code and race are combined to composite key as primary key of DIM_DEMOGRAPHIC|\n",
    "|race|race type: Asian, White, Latino, etc.city_code and state_code and race are combined to composite key as primary key of DIM_DEMOGRAPHIC|\n",
    "|count_each_race| count people in city per race|\n",
    "|median_age|median age of the city in the state|\n",
    "|male_population|male population of the city in the state|\n",
    "|female_population|female population of the city in the state|\n",
    "|total_population|total population of the city in the state|\n",
    "|num_veterans| the number of veterans of the city in the state|\n",
    "|foreign_born| foreign_born of the city in the state|\n",
    "|avg_household_size| average household size of the city in the state|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- FACT_IMMIGRATION|\n",
    "|COLUMN NAME|DESCRIPTION|\n",
    "|-----------|-----------|\n",
    "|cic_id|CIC ID is the primary key of FACT_IMMIGRATION|\n",
    "|city_code|3 digit for the city code that is in United State. city_code and state_code are combined to composite key as foreign key reference to DIM_CITY|\n",
    "|state_code|3 digit for the state code that is in United State. city_code and state_code are combined to composite key as foreign key reference to DIM_CITY|\n",
    "|departure_date|departure date in SAS format|\n",
    "|arrive_date| arrive date in SAS format|\n",
    "|travel_mode| like air, sea, land. only includes air|\n",
    "|visa_type_id| a integer number|\n",
    "|visa_type| includes: WT, B2, CP, etc.|\n",
    "|airline|a 2 digit for airline code|\n",
    "|admin_num| a number string for admin number|\n",
    "|flight_no|flight no.|\n",
    "|birthday|immigrant's birthday in SAS format|\n",
    "|gender|a digit (F or M)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The data model is designed as snowflake schema, with 1 fact and 3 dims. This schema is corresponding with the complex project's data and relationship. In this case, it helps the developers can read and understand the business clearly, and also scale-up easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The project uses both pandas and pyspark because there're the large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The dataset should be rebuilt yearly to get the clear change of analyzing and predicting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- When the data was increased by 100x, We would repartition the data first base on the join key to improve the performance. If this doesn't improve performance that much, we will consider increasing the number of nodes in Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- When the dashboard updates on a daily, we should create an Airflow DAG for it, and use AWS EMR to run the Spark cluster, then use Apache Livy to send Spark commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- When the database need to be accessed by 100+ people, we will scale up the read capacity by using Redshift. We also use AWS IAM to manage users' access to the clusters. Additionally, Amazon Cognito is a good for customizing login page and user management system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
